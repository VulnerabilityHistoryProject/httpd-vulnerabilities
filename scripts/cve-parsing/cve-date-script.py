from bs4 import BeautifulSoup
import requests
import re
import time

#cveName = "CVE-2002-0392"

# reading from cve list file
with open("../listofcves", "r") as cveListFile:
    cveList = []
    for filename in cveListFile:
        print("Reading in CVE entry: " + filename) 
        #! may want to add testing here for bad cve files via regex
        cveList.append(filename)

    for cve in cveList:
        # striping the ".yml" from file name
        cveEntry = cve[0:13]
        print("Writing CVE --> " + cveEntry + " --> to skeleton file." )
        ### Scraping published DATE from page
        url = "https://www.cvedetails.com/cve/" + cveEntry
#testing 
        print("Getting date from  " + url)
        # requests cve page -> FORMAT: url for cve site + cve name
        r = requests.get(url)
        # Check for an invalid cvefile path
        #if r == baseURL:
        #    raise ValueError("missing cve value")
        inputfile = r.text
        soup = BeautifulSoup(inputfile, 'html.parser')
        dateExpression = re.compile("Publish Date")
        dateList = (soup.find_all("span", {"class": "datenote"}))
        dateFound = (str(dateList[0]))

        ## Example:
        # Publish Date :
        # 012345678901234 = 15;index starts @ p
        # date format == xxxx-xx-xx
        #                0123456789 = 10
        publishDateStart = dateFound.find('P') + 15
        publishDateEnd = 10 
        datePayload = dateFound[publishDateStart:(publishDateStart + publishDateEnd)]
       
        # now to find the cve file and insert the newly found date
        print("Opening file --> ../../cves/"+cve[:(len(cve)-1)])
        filePath = "../../cves/" + cve[:(len(cve)-1)]
        with open(filePath, "r+") as outputFile:
            print("Writing -->" +  datePayload + " to respective CVE file")
            lineItem = outputFile.readline()
            while lineItem:
                if lineItem == "announced:":
                    print("Writing: " + "announced: " + datePayload)
                    outputFile.write("announced: " + datePayload)
                    
        # webserver detects scraping, so we will go slower
        time.sleep(3)

